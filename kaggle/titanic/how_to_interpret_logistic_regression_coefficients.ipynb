{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - How to interpret its coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not gonna bore ya with the history of logistic regression, its pervasiveness and popularity among practitioners yada yada yada. \n",
    "\n",
    "Instead, we'll take a *practical* approach to understand how logistic regression works. If you feel I have missed out any details, I'd recommend checking out this <a id='http://statweb.stanford.edu/~tibs/ElemStatLearn/'>book</a>. Without further ado, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Quick Primer](#Quick Primer)\n",
    "2. [Titanic Example](#Titanic Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Primer\n",
    "Logistic Regression is commonly defined as:\n",
    "$$y = \\frac{1}{1+e^{\\Theta^Tx}}$$\n",
    "\n",
    "You already know that, what's more interesting is the above equation can also be interpreted as follows:\n",
    "$$log(\\frac{y}{1-y}) = \\Theta^Tx$$\n",
    "\n",
    "Notice how the linear combination, $\\Theta^T x$, is expressed as the log odds ratio (logit) of $y$, and this will segways well to our next section of how to interpret the coefficients and intercepts from logistic regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic Example\n",
    "\n",
    "<a id='https://www.kaggle.com/c/titanic/data'>Kaggle</a> is a great platform for budding data scientists to get more practice. I'm currently working through the Titanic dataset, and we'll use this as our case study for our logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's load some python libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read into our datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any data crunching, let's clean our data. We'll be interested in the filed Sex later, so let's map the males to the number 0, and females to 1. And we'll separate out the x's and y's into two dataframes for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.Sex = train.Sex.apply(lambda x: 0 if x == 'male' else 1)\n",
    "\n",
    "y_train = train.Survived\n",
    "x_train = train.drop('Survived', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we've cleaned our data, let's feed it through sklearn's <a id=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">logistic regression</a> function to get the coefficients, $\\Theta$, out. Then we'll manually compute the coefficients ourselves to convince ourselves of what's happening.\n",
    "\n",
    "Note: Sklearn applies automatic regularization, so we'll set the parameter $C$ to a large value to emulate no regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaining coefficient of a single dichotomous feature\n",
    "\n",
    "Dichotomous just means the value can only be either 0 or 1, such as the field Sex in our titanic data set. In this section, we'll explore what the coefficients mean when regressing against only one dichotomous feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=1e10)\n",
    "\n",
    "feature = ['Sex']\n",
    "clf.fit(x_train[feature], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fitted the logistic regression function, we can ask sklearn to give us the two terms in $\\Theta$, namely the intercept and the coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('intercept:', clf.intercept_)\n",
    "print('coefficient:', clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With our newly fitted $\\Theta$, now our logistic regression is of the form:\n",
    "$$y = \\frac{1}{1 + e^{-1.45707 + 2.51366x}}$$\n",
    "or\n",
    "$$log(\\frac{y}{1-y}) = -1.45707 + 2.51366x$$\n",
    "\n",
    "So, when $x = 0$, meaning $x = male$, our equation boils down to:\n",
    "$$log(\\frac{p(survived|x=male)}{1-p(survived|x=male)}) = log(\\frac{p(survives|x=male)}{p(\\overline{survive}|x=male)}) = -1.45707$$\n",
    "\n",
    "Exponentiating both sides gives us:\n",
    "$$\\frac{p(survived|x=male)}{p(\\overline{survived}|x=male)} = 0.232917$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check this ourselves with some python magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "survived_by_sex = train[train.Survived == 1].groupby(train.Sex).count()[['Survived']]\n",
    "survived_by_sex['Total'] = train.Survived.groupby(train.Sex).count()\n",
    "survived_by_sex['NotSurvived'] = survived_by_sex.Total - survived_by_sex.Survived\n",
    "survived_by_sex['OddsOfSurvival'] = survived_by_sex.Survived / survived_by_sex.NotSurvived\n",
    "survived_by_sex['ProbOfSurvival'] = survived_by_sex.Survived / survived_by_sex.Total\n",
    "\n",
    "survived_by_sex[['Survived', 'NotSurvived', 'OddsOfSurvival']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for males, we had 109 men who survived, but 468 did not survive. The odds of survival, $\\frac{109}{468} = 0.232906$, which the same as above.\n",
    "\n",
    "If we logged our odds of survival for men, $log(0.232906) = -1.457073$, it's what we got out of our sklearn's intercept component.\n",
    "\n",
    "In essence, the intercept term from the logistic regression is the log odds of our base reference term, which is men who has survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaining coefficient of a single continous feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
