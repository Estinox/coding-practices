{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - How to Interpret Its Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not gonna bore ya with the history of logistic regression, its pervasiveness and popularity among practitioners yada yada yada. \n",
    "\n",
    "Instead, we'll take a *practical* approach to understand how logistic regression works. I'm sure I've skimmed through some details, here's a good <a id='http://statweb.stanford.edu/~tibs/ElemStatLearn/'>book</a> to fill in the blanks. Without further ado, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "* [Quick Primer](#Quick-Primer)\n",
    "* [Titanic Example](#Titanic-Example)\n",
    "  * [Coefficient of a Single Dichotomous Feature](#Coefficient-of-a-Single-Dichotomous-Feature)\n",
    "    * [Fitting Against Sklearn](#Fitting-Against-Sklearn)\n",
    "    * [Survival for Males](#Survival-for-Males)\n",
    "    * [Survival for Females](#Survival-for-Females)\n",
    "* [Where to Go from Here](#Where-to-Go-from-Here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Primer\n",
    "Logistic Regression is commonly defined as:\n",
    "$$h_\\theta(x) = \\frac{1}{1+e^{\\theta^Tx}}$$\n",
    "\n",
    "You already know that, but with some algebriac manipulation, the above equation can also be interpreted as follows\n",
    "$$log\\left(\\frac{h(x)}{1-h(x)}\\right) = \\theta^Tx$$\n",
    "\n",
    "Notice how the linear combination, $\\theta^T x$, is expressed as the *log odds ratio* (logit) of $h(x)$, and let's elaborate on this idea with a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic Example\n",
    "\n",
    "Kaggle is a great platform for budding data scientists to get more practice. I'm currently working through the Titanic dataset, and we'll use this as our case study for our logistic regression.\n",
    "\n",
    "Let's load some python libraries to boot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived                                               Name  \\\n",
       "0            1         0                            Braund, Mr. Owen Harris   \n",
       "1            2         1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2            3         1                             Heikkinen, Miss. Laina   \n",
       "3            4         1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4            5         0                           Allen, Mr. William Henry   \n",
       "\n",
       "      Sex   Age  \n",
       "0    male  22.0  \n",
       "1  female  38.0  \n",
       "2  female  26.0  \n",
       "3  female  35.0  \n",
       "4    male  35.0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train[['PassengerId', 'Survived', 'Name', 'Sex', 'Age']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Coefficient of a Single Dichotomous Feature\n",
    "\n",
    "Dichotomous just means the value can only be either 0 or 1, such as the field Sex in our titanic data set. In this section, we'll explore what the coefficients mean when regressing against only one dichotomous feature.\n",
    "\n",
    "Let's map males to 0, and female to 1, then feed it through sklearn's <a id=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">logistic regression</a> function to get the coefficients out, $\\theta_0$ for the bias, $\\theta_1$ for the logistic coefficient for sex. Then we'll manually compute the coefficients ourselves to convince ourselves of what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.Sex = train.Sex.apply(lambda x: 0 if x == 'male' else 1)\n",
    "\n",
    "y_train = train.Survived\n",
    "x_train = train.drop('Survived', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Fitting Against Sklearn\n",
    "\n",
    "Sklearn applies automatic regularization, so we'll set the parameter $C$ to a large value to emulate no regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000000000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=1e10) # some large number for C\n",
    "\n",
    "feature = ['Sex']\n",
    "clf.fit(x_train[feature], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fitted the logistic regression function, we can ask sklearn to give us the two terms in $\\theta$, namely the intercept and the coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept: [-1.45707193]\n",
      "coefficient: [ 2.51366047]\n"
     ]
    }
   ],
   "source": [
    "print('intercept:', clf.intercept_)\n",
    "print('coefficient:', clf.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cool, so with our newly fitted $\\theta$, now our logistic regression is of the form:\n",
    "$$h(survived | x) = \\frac{1}{1 + e^{(\\theta_0 + \\theta_1x)}} = \\frac{1}{1 + e^{(-1.45707 + 2.51366x)}}$$\n",
    "or\n",
    "$$log\\left(\\frac{h(x)}{1-h(x)}\\right) = -1.45707 + 2.51366x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Survival for Males\n",
    "So, when $x = 0$, meaning $x = male$, our equation boils down to:\n",
    "$$log\\left(\\frac{h(survived|x=male)}{1-h(survived|x=male)}\\right) = log\\left(\\frac{h(survives|x=male)}{h(\\overline{survive}|x=male)}\\right) = -1.45707 + 2.51366(0) = -1.45707$$\n",
    "\n",
    "Exponentiating both sides gives us:\n",
    "$$\\frac{h(survived|x=male)}{h(\\overline{survived}|x=male)} = exp(-1.45707) = 0.232917$$\n",
    "\n",
    "Now let's verify this ourselves via python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Total</th>\n",
       "      <th>NotSurvived</th>\n",
       "      <th>OddsOfSurvival</th>\n",
       "      <th>ProbOfSurvival</th>\n",
       "      <th>Log(OddsOfSurvival)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109</td>\n",
       "      <td>577</td>\n",
       "      <td>468</td>\n",
       "      <td>0.232906</td>\n",
       "      <td>0.188908</td>\n",
       "      <td>-1.457120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233</td>\n",
       "      <td>314</td>\n",
       "      <td>81</td>\n",
       "      <td>2.876543</td>\n",
       "      <td>0.742038</td>\n",
       "      <td>1.056589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Total  NotSurvived  OddsOfSurvival  ProbOfSurvival  \\\n",
       "Sex                                                                 \n",
       "0         109    577          468        0.232906        0.188908   \n",
       "1         233    314           81        2.876543        0.742038   \n",
       "\n",
       "     Log(OddsOfSurvival)  \n",
       "Sex                       \n",
       "0              -1.457120  \n",
       "1               1.056589  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survived_by_sex = train[train.Survived == 1].groupby(train.Sex).count()[['Survived']]\n",
    "survived_by_sex['Total'] = train.Survived.groupby(train.Sex).count()\n",
    "survived_by_sex['NotSurvived'] = survived_by_sex.Total - survived_by_sex.Survived\n",
    "survived_by_sex['OddsOfSurvival'] = survived_by_sex.Survived / survived_by_sex.NotSurvived\n",
    "survived_by_sex['ProbOfSurvival'] = survived_by_sex.Survived / survived_by_sex.Total\n",
    "survived_by_sex['Log(OddsOfSurvival)'] = np.log(survived_by_sex.OddsOfSurvival)\n",
    "\n",
    "survived_by_sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for males, we had 109 men who survived, but 468 did not survive. The odds of survival for men is: $$\\frac{109}{468} = 0.232906$$\n",
    "\n",
    "And if we logged our odds of survival for men:\n",
    "$$log(0.232906) = -1.457$$\n",
    "\n",
    "Which is almost identical to what we have also gotten from skearn's fitting. In essence, **the intercept term from the logistic regression is the log odds of our base reference term**, which is men who has survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Survival for Females\n",
    "\n",
    "Now that we understand what the bias coefficient means in the logistic regression. Naturally, adding $\\theta_1$ gives us the survival probability if female.\n",
    "\n",
    "Don't take my words for it yet, we'll verify that $\\theta_1 = 2.513$ for ourselves.\n",
    "\n",
    "$$ log \\left( \\frac{h(survived | x = female)}{h(\\overline{survived} | x = female)} \\right) = -1.45707 + 2.51366 = 1.05659$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Total</th>\n",
       "      <th>NotSurvived</th>\n",
       "      <th>OddsOfSurvival</th>\n",
       "      <th>ProbOfSurvival</th>\n",
       "      <th>Log(OddsOfSurvival)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>109</td>\n",
       "      <td>577</td>\n",
       "      <td>468</td>\n",
       "      <td>0.232906</td>\n",
       "      <td>0.188908</td>\n",
       "      <td>-1.457120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>233</td>\n",
       "      <td>314</td>\n",
       "      <td>81</td>\n",
       "      <td>2.876543</td>\n",
       "      <td>0.742038</td>\n",
       "      <td>1.056589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Total  NotSurvived  OddsOfSurvival  ProbOfSurvival  \\\n",
       "Sex                                                                 \n",
       "0         109    577          468        0.232906        0.188908   \n",
       "1         233    314           81        2.876543        0.742038   \n",
       "\n",
       "     Log(OddsOfSurvival)  \n",
       "Sex                       \n",
       "0              -1.457120  \n",
       "1               1.056589  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survived_by_sex # same table as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 314 female on board, 233 survived, but 81 didn't. So the odds of survival for females is:\n",
    "$$ \\frac{233}{81} = 2.876$$\n",
    "\n",
    "Then taking the log both sides gives us:\n",
    "$$ log(2.876) = 1.056$$\n",
    "\n",
    "And this is also what sklearn gave us as its $\\theta^Tx$ from above.\n",
    "\n",
    "Voila, nothing too ground breaking. Although it's interesting to understand the relationship between $h(\\theta^Tx)$ and $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Go from Here\n",
    "\n",
    "\n",
    "If you're still trying to make more connections to how the logistic regression is derived, I would point you in the direction of the Bernoulli distribution, how the bernoulli can be expressed as part of the exponential family, and how Generalized Linear Model can produces a learning algorithm for all members of the exponential family. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
